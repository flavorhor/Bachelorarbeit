{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel in PyTorch, das eine kleine, auf Steganalyse angepasste Architektur zeigt. Sie kombiniert einen (optionalen) High-Pass-Filter mit Convolution-Blöcken und (optional) Residual-Blöcken. Dies ist keine „Abschrift“ eines offiziellen ResNet, sondern eher ein Residual-Ansatz in kompakter Form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Falls du train_test_split aus sklearn bevorzugst (statt random_split):\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset und DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassen zu Indizes: {'clean': 0, 'stego': 1}\n",
      "Total images: 70000\n"
     ]
    }
   ],
   "source": [
    "# Pfad zum Datenordner (wo 'clean' und 'stego' liegen)\n",
    "data_dir = r\"C:\\Users\\Flavio\\Bachelorarbeit\\LSB\\data_LSB\"\n",
    "\n",
    "# Transformation: \n",
    "# 1) Convert to Grayscale (macht aus RGB -> 1-Kanal, falls PNGs mit 3 Kanälen existieren)\n",
    "# 2) Resize auf (28,28)\n",
    "# 3) ToTensor() -> Tensor [C,H,W] im Bereich [0,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset: ImageFolder erwartet Unterordner (clean, stego).\n",
    "# Dabei bekommt \"clean\" Label=0, \"stego\" Label=1 (alphabetische Sortierung)\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "print(\"Klassen zu Indizes:\", full_dataset.class_to_idx)\n",
    "print(\"Total images:\", len(full_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-/Val-/Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56000 Val: 7000 Test: 7000\n"
     ]
    }
   ],
   "source": [
    "# Beispielhafter Split: 80% Train, 10% Val, 10% Test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size   = int(0.1 * len(full_dataset))\n",
    "test_size  = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # für reproduzierbare Splits\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset), \"Test:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High-Pass-Filter-Layer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighpassConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HighpassConv, self).__init__()\n",
    "        # Ein Beispiel-Laplacian Filter (3x3); \n",
    "        # Hinweis: für 1-channel Bilder. Bei mehr Kanälen ggf. anpassen.\n",
    "        kernel = torch.tensor([[0, -1, 0],\n",
    "                               [-1, 4, -1],\n",
    "                               [0, -1, 0]], dtype=torch.float32)\n",
    "        # Wir initialisieren einen 1x1-Conv-Layer mit diesem Filter als festen Filter\n",
    "        self.conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "        # Setze die Gewichte und friere sie ein (optional: später fein-tunbar machen)\n",
    "        self.conv.weight.data = kernel.unsqueeze(0).unsqueeze(0)\n",
    "        self.conv.weight.requires_grad = False  # Wenn du fein-tuning möchtest, setze dies auf True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Residual Block (vereinfacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation=1, use_batchnorm=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        padding = dilation  # um die räumliche Auflösung zu erhalten\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(channels) if use_batchnorm else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(channels) if use_batchnorm else nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Das Hauptmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteganoNet(nn.Module):\n",
    "    def __init__(self, use_highpass=True):\n",
    "        super(SteganoNet, self).__init__()\n",
    "        self.use_highpass = use_highpass\n",
    "\n",
    "        # Falls der Highpass-Filter genutzt werden soll, verarbeiten wir zuerst diesen\n",
    "        if self.use_highpass:\n",
    "            self.highpass = HighpassConv()\n",
    "        else:\n",
    "            self.highpass = nn.Identity()\n",
    "\n",
    "        # Erste Convolution, um die Kanaldimension auf einen höheren Wert zu bringen (z.B. 32)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Mehrere Residual-Blöcke; evtl. mit unterschiedlichen Dilatationsfaktoren, \n",
    "        # um Informationen auf verschiedenen Skalen zu erfassen\n",
    "        self.resblock1 = ResidualBlock(32, dilation=1)\n",
    "        self.resblock2 = ResidualBlock(32, dilation=1)\n",
    "        self.resblock3 = ResidualBlock(32, dilation=2)\n",
    "        self.resblock4 = ResidualBlock(32, dilation=2)\n",
    "        \n",
    "        # Um Überanpassung zu vermeiden, kann hier ein Dropout nützlich sein.\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Da die räumlichen Dimensionen klein sind (28x28), verzichten wir auf umfangreiches Pooling.\n",
    "        # Eine leichte Reduktion kann allerdings helfen, globale Zusammenhänge zu integrieren.\n",
    "        # Hier ein strided conv, der die Auflösung halbiert.\n",
    "        self.downsample = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn_down = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Weitere Residual Blöcke im kleineren (64-Channel) Feature-Raum\n",
    "        self.resblock5 = ResidualBlock(64, dilation=1)\n",
    "        self.resblock6 = ResidualBlock(64, dilation=1)\n",
    "        \n",
    "        # Global Average Pooling, um von 2D-Feature-Maps auf einen Vektor zu kommen\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Abschließende Klassifikation: Da es hier zwei Klassen gibt (stego vs. normal)\n",
    "        self.fc = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Optionaler Highpass-Filter: betont feine Unterschiede\n",
    "        x = self.highpass(x)\n",
    "        \n",
    "        # Erste Feature-Extraktion\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Mehrere Residual-Blöcke (ohne Änderung der Auflösung)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        \n",
    "        # Optional: Dropout, um das Netz zu regulieren\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Downsampling, aber nur in einem moderaten Schritt, um globale Information einzufangen\n",
    "        x = self.relu(self.bn_down(self.downsample(x)))\n",
    "        \n",
    "        # Weitere Residual-Blöcke im kleineren, aber reicheren Feature-Raum\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        \n",
    "        # Global Average Pooling zur Reduktion auf einen Feature-Vektor\n",
    "        x = self.global_pool(x)  # Ergebnis: [Batch, 64, 1, 1]\n",
    "        x = torch.flatten(x, 1)  # Ergebnis: [Batch, 64]\n",
    "        x = self.fc(x)           # Ergebnis: [Batch, 2] (logits für 2 Klassen)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training & Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0218 | Train Accuracy: 0.9880\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0005 | Train Accuracy: 0.9999\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0003 | Train Accuracy: 0.9999\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 | Train Accuracy: 1.0000\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 | Train Accuracy: 1.0000\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 | Train Accuracy: 1.0000\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000 | Train Accuracy: 1.0000\n",
      "Val Loss: 0.0000 | Val Accuracy: 1.0000\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Backward-Pass und Optimierung\u001b[39;00m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Metriken aktualisieren\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Flavio\\Bachelorarbeit\\venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Flavio\\Bachelorarbeit\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Flavio\\Bachelorarbeit\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Modell und Loss definieren\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SteganoNet(use_highpass=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 20\n",
    "\n",
    "# 4. Training und Validierung\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward-Pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward-Pass und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metriken aktualisieren\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward-Pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Metriken aktualisieren\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Haupt-Trainingsloop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testen auf dem Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000, Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x = test_x.to(device)\n",
    "        test_y = test_y.to(device)\n",
    "        \n",
    "        outputs = model(test_x)\n",
    "        loss_val = criterion(outputs, test_y)\n",
    "        test_loss += loss_val.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bild-Index: 0  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 1  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 2  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 3  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 4  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 5  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 6  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 7  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 8  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 9  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 10  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 11  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 12  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 13  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 14  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 15  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 16  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 17  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 18  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 19  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 20  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 21  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 22  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 23  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 24  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 25  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 26  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 27  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 28  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 29  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 30  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 31  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 32  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 33  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 34  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 35  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 36  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 37  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 38  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 39  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 40  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 41  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 42  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 43  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 44  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 45  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 46  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 47  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 48  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 49  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 50  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 51  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 52  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 53  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 54  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 55  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 56  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 57  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 58  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 59  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 60  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 61  ->  True Label: 0  |  Vorhersage: 0  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 62  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n",
      "Bild-Index: 63  ->  True Label: 1  |  Vorhersage: 1  |  Wahrscheinlichkeit: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Setzt das Modell in den Evaluierungsmodus\n",
    "all_predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)  # Rohe Logits des Modells\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Wahrscheinlichkeiten für jede Klasse\n",
    "        preds = probabilities.argmax(dim=1)  # Vorhersagen: Index der höchsten Wahrscheinlichkeit\n",
    "        \n",
    "        # Speichere Ergebnisse in einem Dictionary\n",
    "        for i, (prob, pred, label) in enumerate(zip(probabilities, preds, labels)):\n",
    "            all_predictions[i] = {\n",
    "                \"true_label\": label.item(),\n",
    "                \"predicted_label\": pred.item(),\n",
    "                \"probability\": prob[pred].item(),  # Wahrscheinlichkeit der vorhergesagten Klasse\n",
    "            }\n",
    "\n",
    "# Ausgabe aller Bilder, Labels und Vorhersagen\n",
    "for idx, result in all_predictions.items():\n",
    "    print(\n",
    "        f\"Bild-Index: {idx}  ->  \"\n",
    "        f\"True Label: {result['true_label']}  |  \"\n",
    "        f\"Vorhersage: {result['predicted_label']}  |  \"\n",
    "        f\"Wahrscheinlichkeit: {result['probability']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wichtige Anmerkungen für Steganalyse:\n",
    "\n",
    "    Daten-Augmentierung:\n",
    "        Für Steganalyse oft nur minimal (z. B. leichte Übersetzung, dezente Helligkeitsschwankungen).\n",
    "        Starke Geometrie-Transformationen (Rotation, Flip) könnten LSB-Informationen verfälschen.\n",
    "\n",
    "    High-Pass vs. trainierbare Filter:\n",
    "        Man kann den HighPassFilter durch eine trainierbare erste Convolution-Schicht ersetzen (mit kleiner Kernel-Größe, z. B. 3×3) und ggf. den Bias weglassen.\n",
    "        Das Netz lernt dann eigenständig, den optimalen Filter zu finden.\n",
    "\n",
    "    Pooling (Downsampling) vs. Erhaltung feinster Details:\n",
    "        Zu viele Pooling-Schritte können die subtilen LSB-Muster verwässern. 2–3 Poolings könnten schon viel sein bei nur 28×28 Pixeln. Ggf. also nur 2× MaxPool oder stattdessen Strided Convolution.\n",
    "        Alternativ kann man in späteren Blöcken Global Average Pooling verwenden und in den ersten Schritten weniger (oder kein) Pooling.\n",
    "\n",
    "    Evaluierung mit Precision/Recall:\n",
    "        Gerade bei Steganalyse kann es sein, dass das Klassifikationsproblem unbalanced oder asymmetrisch in der Fehlerbewertung ist (z. B. false negatives = gefährlicher).\n",
    "        Messe daher nicht nur Accuracy, sondern auch Precision/Recall/F1-Score.\n",
    "\n",
    "    Hyperparametertuning:\n",
    "        Größe und Anzahl der Filter\n",
    "        Anzahl ResidualBlöcke\n",
    "        Lernrate, Weight Decay, Dropout etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch nicht (oder nur rudimentär) umgesetzt\n",
    "\n",
    "    Curriculum Training\n",
    "\n",
    "    Early Stopping\n",
    "        kein Early-Stopping-Mechanismus (z. B. Abbruch des Trainings, wenn sich der Validierungs-Loss nicht mehr verbessert) implementiert. Du könntest das über ein Callback-ähnliches Konstrukt oder eine Abbruchlogik leicht ergänzen.\n",
    "\n",
    "    Learning-Rate-Scheduler\n",
    "        Lernrate nicht dynamisch angepasst. Ein Scheduler (z. B. StepLR, ReduceLROnPlateau usw.) könnte das Training verbessern.\n",
    "\n",
    "    Ausführliche Metriken (Precision/Recall/F1-Score)\n",
    "        nur Accuracy während des Validierungslaufs ausgegeben. Für Steganalyse lohnt es sich, zusätzlich Precision, Recall, F1 und eine Confusion Matrix zu berechnen.\n",
    "\n",
    "    Cross-Validation\n",
    "        nutzt eine klassische Train-/Validation-Aufteilung. Eine mehrfache (z. B. 5-fach) Cross-Validation ist nicht implementiert und müsste manuell oder über Hilfsbibliotheken (z. B. sklearn.model_selection.KFold) hinzugefügt werden.\n",
    "\n",
    "    Hyperparameter-Tuning\n",
    "        Architektur und Parameter (z. B. Anzahl Filter, Pooling etc.) festgelegt. Ein eigentliches Tuning (systematisches Variieren der Parameter) fehlt\n",
    "\n",
    "    Deployment-Aspekte\n",
    "        keine Hinweise zu Inference-Geschwindigkeit, Onnx-Export, oder Embedded-Anwendungen. Separater Schritt nach erfolgreichem Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

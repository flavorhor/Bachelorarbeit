{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel in PyTorch, das eine kleine, auf Steganalyse angepasste Architektur zeigt. Sie kombiniert einen (optionalen) High-Pass-Filter mit Convolution-Blöcken und (optional) Residual-Blöcken. Dies ist keine „Abschrift“ eines offiziellen ResNet, sondern eher ein Residual-Ansatz in kompakter Form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "# Falls du train_test_split aus sklearn bevorzugst (statt random_split):\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset und DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassen zu Indizes: {'clean': 0, 'stego': 1}\n",
      "Total images: 70000\n"
     ]
    }
   ],
   "source": [
    "# Pfad zum Datenordner (wo 'clean' und 'stego' liegen)\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Transformation: \n",
    "# 1) Convert to Grayscale (macht aus RGB -> 1-Kanal, falls PNGs mit 3 Kanälen existieren)\n",
    "# 2) Resize auf (28,28)\n",
    "# 3) ToTensor() -> Tensor [C,H,W] im Bereich [0,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset: ImageFolder erwartet Unterordner (clean, stego).\n",
    "# Dabei bekommt \"clean\" Label=0, \"stego\" Label=1 (alphabetische Sortierung)\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "print(\"Klassen zu Indizes:\", full_dataset.class_to_idx)\n",
    "print(\"Total images:\", len(full_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-/Val-/Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56000 Val: 7000 Test: 7000\n"
     ]
    }
   ],
   "source": [
    "# Beispielhafter Split: 80% Train, 10% Val, 10% Test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size   = int(0.1 * len(full_dataset))\n",
    "test_size  = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # für reproduzierbare Splits\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset), \"Test:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High-Pass-Filter-Layer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighPassFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Ein einfacher, nicht-trainierbarer High-Pass-Filterschritt.\n",
    "    Beispielkern (Laplacian).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HighPassFilter, self).__init__()\n",
    "        # Laplace-Kernel 3x3\n",
    "        kernel = torch.tensor(\n",
    "            [[-1., -1., -1.],\n",
    "             [-1.,  8., -1.],\n",
    "             [-1., -1., -1.]]\n",
    "        ).reshape((1,1,3,3))  # shape: (out_channels, in_channels, kH, kW)\n",
    "        \n",
    "        self.weight = nn.Parameter(kernel, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x hat shape (B, C=1, H, W)\n",
    "        # Wir wenden den Filter Kanal für Kanal an; hier haben wir 1 Kanal\n",
    "        return F.conv2d(x, self.weight, stride=1, padding=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Residual Block (vereinfacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Vereinfachter ResNet-artiger Block:\n",
    "    - Zwei Conv(3x3) + BatchNorm + ReLU\n",
    "    - Skip Connection\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # Skip\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Das Hauptmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "class StegoNet(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(StegoNet, self).__init__()\n",
    "        \n",
    "        # 1) Optional: High-Pass-Filter als Eingangsschicht\n",
    "        self.highpass = HighPassFilter()\n",
    "        \n",
    "        # 2) Erstes Conv -> wir starten mit 32 Filtern\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # 3) ResidualBlock 1\n",
    "        self.resblock1 = ResidualBlock(32)\n",
    "        \n",
    "        # 4) Downsampling (kleines Pooling)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # halbiert H und W\n",
    "        \n",
    "        # 5) Zweite Conv-Ebene (auf z.B. 64 Kanäle erhöhen)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 6) ResidualBlock 2\n",
    "        self.resblock2 = ResidualBlock(64)\n",
    "        \n",
    "        # 7) Wieder Downsampling\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # halbiert H und W erneut\n",
    "        \n",
    "        # 8) Optional: Dritte Ebene\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # ResidualBlock 3\n",
    "        self.resblock3 = ResidualBlock(128)\n",
    "        \n",
    "        # 9) Wieder Downsampling (je nach Bedarf)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)  # Bei 28x28 -> nach 3 Poolings = 3x3\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binäre Klassifikation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Optionaler High-Pass-Filter\n",
    "        x = self.highpass(x)\n",
    "\n",
    "        # Convolution 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Residual Block 1\n",
    "        x = self.resblock1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Convolution 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # Residual Block 2\n",
    "        x = self.resblock2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Convolution 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Residual Block 3\n",
    "        x = self.resblock3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # B, (128*3*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Sigmoid für binäre Klassifikation\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training & Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.6955, Val Loss: 0.6932, Val Acc: 0.4961\n",
      "Epoch [2/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [3/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [4/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [5/20] Train Loss: 0.6932, Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch [6/20] Train Loss: 0.6935, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [7/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [8/20] Train Loss: 0.6932, Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch [9/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [10/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [11/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [12/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [13/20] Train Loss: 0.6932, Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch [14/20] Train Loss: 0.6932, Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch [15/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [16/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [17/20] Train Loss: 0.6932, Val Loss: 0.6932, Val Acc: 0.4960\n",
      "Epoch [18/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [19/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n",
      "Epoch [20/20] Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5040\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere Modell\n",
    "model = StegoNet(in_channels=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss und Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Optional: Falls du Weight Decay verwenden willst:\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 20  # Du kannst natürlich mehr trainieren (z. B. 50-100)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.float().to(device)  # Labels: 0 oder 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y.unsqueeze(1))  \n",
    "        # batch_y hat shape [B], für BCELoss wollen wir [B,1] => unsqueeze(1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in val_loader:\n",
    "            val_x = val_x.to(device)\n",
    "            val_y = val_y.float().to(device)\n",
    "            \n",
    "            val_outputs = model(val_x)\n",
    "            loss_val = criterion(val_outputs, val_y.unsqueeze(1))\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            preds = (val_outputs >= 0.5).long()\n",
    "            correct += (preds.squeeze(1) == val_y).sum().item()\n",
    "            total += val_y.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    epoch_val_loss   = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testen auf dem Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6933, Test Accuracy: 0.4899\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x = test_x.to(device)\n",
    "        test_y = test_y.float().to(device)\n",
    "        \n",
    "        outputs = model(test_x)\n",
    "        loss_val = criterion(outputs, test_y.unsqueeze(1))\n",
    "        test_loss += loss_val.item()\n",
    "        \n",
    "        preds = (outputs >= 0.5).long()\n",
    "        correct += (preds.squeeze(1) == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wichtige Anmerkungen für Steganalyse:\n",
    "\n",
    "    Daten-Augmentierung:\n",
    "        Für Steganalyse oft nur minimal (z. B. leichte Übersetzung, dezente Helligkeitsschwankungen).\n",
    "        Starke Geometrie-Transformationen (Rotation, Flip) könnten LSB-Informationen verfälschen.\n",
    "\n",
    "    High-Pass vs. trainierbare Filter:\n",
    "        Man kann den HighPassFilter durch eine trainierbare erste Convolution-Schicht ersetzen (mit kleiner Kernel-Größe, z. B. 3×3) und ggf. den Bias weglassen.\n",
    "        Das Netz lernt dann eigenständig, den optimalen Filter zu finden.\n",
    "\n",
    "    Pooling (Downsampling) vs. Erhaltung feinster Details:\n",
    "        Zu viele Pooling-Schritte können die subtilen LSB-Muster verwässern. 2–3 Poolings könnten schon viel sein bei nur 28×28 Pixeln. Ggf. also nur 2× MaxPool oder stattdessen Strided Convolution.\n",
    "        Alternativ kann man in späteren Blöcken Global Average Pooling verwenden und in den ersten Schritten weniger (oder kein) Pooling.\n",
    "\n",
    "    Evaluierung mit Precision/Recall:\n",
    "        Gerade bei Steganalyse kann es sein, dass das Klassifikationsproblem unbalanced oder asymmetrisch in der Fehlerbewertung ist (z. B. false negatives = gefährlicher).\n",
    "        Messe daher nicht nur Accuracy, sondern auch Precision/Recall/F1-Score.\n",
    "\n",
    "    Hyperparametertuning:\n",
    "        Größe und Anzahl der Filter\n",
    "        Anzahl ResidualBlöcke\n",
    "        Lernrate, Weight Decay, Dropout etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch nicht (oder nur rudimentär) umgesetzt\n",
    "\n",
    "    Curriculum Training\n",
    "\n",
    "    Early Stopping\n",
    "        kein Early-Stopping-Mechanismus (z. B. Abbruch des Trainings, wenn sich der Validierungs-Loss nicht mehr verbessert) implementiert. Du könntest das über ein Callback-ähnliches Konstrukt oder eine Abbruchlogik leicht ergänzen.\n",
    "\n",
    "    Learning-Rate-Scheduler\n",
    "        Lernrate nicht dynamisch angepasst. Ein Scheduler (z. B. StepLR, ReduceLROnPlateau usw.) könnte das Training verbessern.\n",
    "\n",
    "    Ausführliche Metriken (Precision/Recall/F1-Score)\n",
    "        nur Accuracy während des Validierungslaufs ausgegeben. Für Steganalyse lohnt es sich, zusätzlich Precision, Recall, F1 und eine Confusion Matrix zu berechnen.\n",
    "\n",
    "    Cross-Validation\n",
    "        nutzt eine klassische Train-/Validation-Aufteilung. Eine mehrfache (z. B. 5-fach) Cross-Validation ist nicht implementiert und müsste manuell oder über Hilfsbibliotheken (z. B. sklearn.model_selection.KFold) hinzugefügt werden.\n",
    "\n",
    "    Hyperparameter-Tuning\n",
    "        Architektur und Parameter (z. B. Anzahl Filter, Pooling etc.) festgelegt. Ein eigentliches Tuning (systematisches Variieren der Parameter) fehlt\n",
    "\n",
    "    Deployment-Aspekte\n",
    "        keine Hinweise zu Inference-Geschwindigkeit, Onnx-Export, oder Embedded-Anwendungen. Separater Schritt nach erfolgreichem Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel in PyTorch, das eine kleine, auf Steganalyse angepasste Architektur zeigt. Sie kombiniert einen (optionalen) High-Pass-Filter mit Convolution-Blöcken und (optional) Residual-Blöcken. Dies ist keine „Abschrift“ eines offiziellen ResNet, sondern eher ein Residual-Ansatz in kompakter Form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "# Falls du train_test_split aus sklearn bevorzugst (statt random_split):\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset und DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassen zu Indizes: {'clean': 0, 'stego': 1}\n",
      "Total images: 70000\n"
     ]
    }
   ],
   "source": [
    "# Pfad zum Datenordner (wo 'clean' und 'stego' liegen)\n",
    "data_dir = \"/Users/flaviohorak/Desktop/Bachelorarbeit/notebooks/createLSB/data\"\n",
    "\n",
    "# Transformation: \n",
    "# 1) Convert to Grayscale (macht aus RGB -> 1-Kanal, falls PNGs mit 3 Kanälen existieren)\n",
    "# 2) Resize auf (28,28)\n",
    "# 3) ToTensor() -> Tensor [C,H,W] im Bereich [0,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset: ImageFolder erwartet Unterordner (clean, stego).\n",
    "# Dabei bekommt \"clean\" Label=0, \"stego\" Label=1 (alphabetische Sortierung)\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "print(\"Klassen zu Indizes:\", full_dataset.class_to_idx)\n",
    "print(\"Total images:\", len(full_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train-/Val-/Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56000 Val: 7000 Test: 7000\n"
     ]
    }
   ],
   "source": [
    "# Beispielhafter Split: 80% Train, 10% Val, 10% Test\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size   = int(0.1 * len(full_dataset))\n",
    "test_size  = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # für reproduzierbare Splits\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Val:\", len(val_dataset), \"Test:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High-Pass-Filter-Layer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighPassFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Ein einfacher, nicht-trainierbarer High-Pass-Filterschritt.\n",
    "    Beispielkern (Laplacian).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HighPassFilter, self).__init__()\n",
    "        # Laplace-Kernel 3x3\n",
    "        kernel = torch.tensor(\n",
    "            [[-1., -1., -1.],\n",
    "             [-1.,  8., -1.],\n",
    "             [-1., -1., -1.]]\n",
    "        ).reshape((1,1,3,3))  # shape: (out_channels, in_channels, kH, kW)\n",
    "        \n",
    "        self.weight = nn.Parameter(kernel, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x hat shape (B, C=1, H, W)\n",
    "        # Wir wenden den Filter Kanal für Kanal an; hier haben wir 1 Kanal\n",
    "        return F.conv2d(x, self.weight, stride=1, padding=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Residual Block (vereinfacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Vereinfachter ResNet-artiger Block:\n",
    "    - Zwei Conv(3x3) + BatchNorm + ReLU\n",
    "    - Skip Connection\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # Skip\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Das Hauptmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegoNet(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(StegoNet, self).__init__()\n",
    "        \n",
    "        # 1) Optional: High-Pass-Filter als Eingangsschicht\n",
    "        self.highpass = HighPassFilter()\n",
    "        \n",
    "        # 2) Erstes Conv -> wir starten mit 32 Filtern\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # 3) ResidualBlock 1\n",
    "        self.resblock1 = ResidualBlock(32)\n",
    "        \n",
    "        # 4) Downsampling (kleines Pooling)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # halbiert H und W\n",
    "        \n",
    "        # 5) Zweite Conv-Ebene (auf z.B. 64 Kanäle erhöhen)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 6) ResidualBlock 2\n",
    "        self.resblock2 = ResidualBlock(64)\n",
    "        \n",
    "        # 7) Wieder Downsampling\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # halbiert H und W erneut\n",
    "        \n",
    "        # 8) Optional: Dritte Ebene\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # ResidualBlock 3\n",
    "        self.resblock3 = ResidualBlock(128)\n",
    "        \n",
    "        # 9) Wieder Downsampling (je nach Bedarf)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)  # Bei 28x28 -> nach 3 Poolings = 3x3\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binäre Klassifikation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Optionaler High-Pass-Filter\n",
    "        x = self.highpass(x)\n",
    "\n",
    "        # Convolution 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Residual Block 1\n",
    "        x = self.resblock1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Convolution 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # Residual Block 2\n",
    "        x = self.resblock2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Convolution 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Residual Block 3\n",
    "        x = self.resblock3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # B, (128*3*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Sigmoid für binäre Klassifikation\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training & Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Labels: 0 oder 1\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))  \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# batch_y hat shape [B], für BCELoss wollen wir [B,1] => unsqueeze(1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36mStegoNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhighpass(x)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Convolution 1\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Residual Block 1\u001b[39;00m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblock1(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialisiere Modell\n",
    "model = StegoNet(in_channels=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss und Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Optional: Falls du Weight Decay verwenden willst:\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 20  # Du kannst natürlich mehr trainieren (z. B. 50-100)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.float().to(device)  # Labels: 0 oder 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y.unsqueeze(1))  \n",
    "        # batch_y hat shape [B], für BCELoss wollen wir [B,1] => unsqueeze(1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_x, val_y in val_loader:\n",
    "            val_x = val_x.to(device)\n",
    "            val_y = val_y.float().to(device)\n",
    "            \n",
    "            val_outputs = model(val_x)\n",
    "            loss_val = criterion(val_outputs, val_y.unsqueeze(1))\n",
    "            val_loss += loss_val.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            preds = (val_outputs >= 0.5).long()\n",
    "            correct += (preds.squeeze(1) == val_y).sum().item()\n",
    "            total += val_y.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    epoch_val_loss   = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testen auf dem Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x = test_x.to(device)\n",
    "        test_y = test_y.float().to(device)\n",
    "        \n",
    "        outputs = model(test_x)\n",
    "        loss_val = criterion(outputs, test_y.unsqueeze(1))\n",
    "        test_loss += loss_val.item()\n",
    "        \n",
    "        preds = (outputs >= 0.5).long()\n",
    "        correct += (preds.squeeze(1) == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wichtige Anmerkungen für Steganalyse:\n",
    "\n",
    "    Daten-Augmentierung:\n",
    "        Für Steganalyse oft nur minimal (z. B. leichte Übersetzung, dezente Helligkeitsschwankungen).\n",
    "        Starke Geometrie-Transformationen (Rotation, Flip) könnten LSB-Informationen verfälschen.\n",
    "\n",
    "    High-Pass vs. trainierbare Filter:\n",
    "        Man kann den HighPassFilter durch eine trainierbare erste Convolution-Schicht ersetzen (mit kleiner Kernel-Größe, z. B. 3×3) und ggf. den Bias weglassen.\n",
    "        Das Netz lernt dann eigenständig, den optimalen Filter zu finden.\n",
    "\n",
    "    Pooling (Downsampling) vs. Erhaltung feinster Details:\n",
    "        Zu viele Pooling-Schritte können die subtilen LSB-Muster verwässern. 2–3 Poolings könnten schon viel sein bei nur 28×28 Pixeln. Ggf. also nur 2× MaxPool oder stattdessen Strided Convolution.\n",
    "        Alternativ kann man in späteren Blöcken Global Average Pooling verwenden und in den ersten Schritten weniger (oder kein) Pooling.\n",
    "\n",
    "    Evaluierung mit Precision/Recall:\n",
    "        Gerade bei Steganalyse kann es sein, dass das Klassifikationsproblem unbalanced oder asymmetrisch in der Fehlerbewertung ist (z. B. false negatives = gefährlicher).\n",
    "        Messe daher nicht nur Accuracy, sondern auch Precision/Recall/F1-Score.\n",
    "\n",
    "    Hyperparametertuning:\n",
    "        Größe und Anzahl der Filter\n",
    "        Anzahl ResidualBlöcke\n",
    "        Lernrate, Weight Decay, Dropout etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch nicht (oder nur rudimentär) umgesetzt\n",
    "\n",
    "    Early Stopping\n",
    "        Im gezeigten Code wird noch kein Early-Stopping-Mechanismus (z. B. Abbruch des Trainings, wenn sich der Validierungs-Loss nicht mehr verbessert) implementiert. Du könntest das über ein Callback-ähnliches Konstrukt oder eine Abbruchlogik leicht ergänzen.\n",
    "\n",
    "    Learning-Rate-Scheduler\n",
    "        Im Beispielcode wird die Lernrate nicht dynamisch angepasst. Ein Scheduler (z. B. StepLR, ReduceLROnPlateau usw.) könnte das Training verbessern.\n",
    "\n",
    "    Ausführliche Metriken (Precision/Recall/F1-Score)\n",
    "        Aktuell wird nur Accuracy während des Validierungslaufs ausgegeben. Für Steganalyse lohnt es sich, zusätzlich Precision, Recall, F1 und eine Confusion Matrix zu berechnen.\n",
    "\n",
    "    Cross-Validation\n",
    "        Das Beispiel nutzt eine klassische Train-/Validation-Aufteilung. Eine mehrfache (z. B. 5-fach) Cross-Validation ist nicht implementiert und müsste manuell oder über Hilfsbibliotheken (z. B. sklearn.model_selection.KFold) hinzugefügt werden.\n",
    "\n",
    "    Gewichtetes Training bei unbalanced Datensätzen\n",
    "        Falls die Datensätze unbalanced sind (z. B. viel mehr normale Bilder als Stego-Bilder), haben wir keine Gewichtungen oder Sampler angewendet. Das ließe sich über WeightedRandomSampler o. Ä. nachrüsten.\n",
    "\n",
    "    Hyperparameter-Tuning\n",
    "        Im Code selbst haben wir die Architektur und Parameter (z. B. Anzahl Filter, Pooling etc.) festgelegt. Ein eigentliches Tuning (systematisches Variieren der Parameter) ist nicht implementiert, sondern dir überlassen.\n",
    "\n",
    "    Deployment-Aspekte\n",
    "        Wir haben keine Hinweise zu Inference-Geschwindigkeit, Onnx-Export, oder Embedded-Anwendungen. Das wäre ein separater Schritt nach erfolgreichem Training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
